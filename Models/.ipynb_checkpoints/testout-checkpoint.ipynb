{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit Logistic Model\n",
    "from logisticmodel import logisticmodel\n",
    "y_test, y_pred = logisticmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.astype('float')\n",
    "y_test = y_test.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating the Model using the Testing Dataset\n",
    "# Plot normalized confusion matrix\n",
    "labels = ['50-60', '60-70', '70-80', '80-90', '90-100', '100-110', '110-120', '120-130', '130-140', '140-150', '150-160', '160-170', '170+']\n",
    "from plot_conf_matrix import plot_confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.26 0.   0.05 0.23 0.05 0.15 0.13 0.08 0.05 0.   0.   0.   0.  ]\n",
      " [0.21 0.   0.08 0.31 0.01 0.14 0.08 0.11 0.06 0.   0.   0.   0.  ]\n",
      " [0.02 0.   0.07 0.19 0.08 0.17 0.31 0.12 0.03 0.   0.   0.   0.  ]\n",
      " [0.01 0.   0.08 0.22 0.04 0.21 0.22 0.13 0.1  0.   0.   0.   0.  ]\n",
      " [0.01 0.   0.03 0.16 0.03 0.21 0.28 0.18 0.09 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.02 0.06 0.02 0.17 0.31 0.28 0.13 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.04 0.01 0.09 0.34 0.35 0.16 0.01 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.02 0.   0.07 0.27 0.4  0.23 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.01 0.   0.04 0.18 0.37 0.4  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.05 0.16 0.34 0.45 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.01 0.13 0.27 0.59 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.11 0.18 0.68 0.04 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.05 0.15 0.3  0.5  0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "fig, ax = plt.subplots( nrows=1, ncols=1 , figsize=(8, 8)) \n",
    "plot_confusion_matrix(cnf_matrix, normalize = True, classes=labels, title='Normalized Confusion Matrix')\n",
    "fig.savefig('norm_conf_matrix.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = np.asmatrix(cm)\n",
    "tp = np.trace(cm)\n",
    "fn = np.triu(cm).sum()-np.trace(cm)\n",
    "fp = np.tril(cm).sum()-np.trace(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.38\n",
      "Recall 0.36\n"
     ]
    }
   ],
   "source": [
    "# Precision (if we want to minimize false positives)\n",
    "precision = tp / (tp + fp)\n",
    "print(\"Precision {:0.2f}\".format(precision))\n",
    "\n",
    "# Recall (least false negatives)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"Recall {:0.2f}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score 0.37\n",
      "RMSE 19.94\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "# Harmonic mean of PR, used to indicate a balance between \n",
    "# PR providing each equal weightage, it ranges from 0 to 1. \n",
    "# F1 Score reaches its best value at 1 (perfect PR) and worst at 0.\n",
    "# Relations between dataâ€™s positive labels and those given by a classifier based on sums of per-text decisions\n",
    "f1 = (2*precision*recall)/(precision + recall)\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "\n",
    "### Calculate RSME\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(y_test.astype(np.float), y_pred.astype(np.float)))\n",
    "print(\"RMSE {:0.2f}\".format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
